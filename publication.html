<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Publication</title>
        <link rel="shortcut icon" href="./octopus.ico?" type="image/x-icon">
        
    </head>

    <body>
    	<div class="publication">
    	    <img class="small-octopus-img" src="octopus.png"/>
    	    <h2>PUBLICATION</h2>
    	    <div class="publication-list">
                <ul>
                <li>Huiyin Xue, Nafise Sadat Moosavi, Nikolaos Aletras. 2025. Deconstructing Attention: Investigating Design Principles for Effective Language Modeling. In <i>Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics 2025</i>, pages 708–727, Mumbai, India. The Asian Federation of Natural Language Processing and The Association for Computational Linguistics.
                    <p><a href="aacl25_Poster_.pdf" target="blank">[poster]</a> <a href="https://aclanthology.org/2025.ijcnlp-long.40/">[pdf]</a><p>
                </li>

                <li>Milan Gritta*, Huiyin Xue*, Gerasimos Lampouras. 2023. DReSD: Dense Retrieval for Speculative Decoding. In <i>Findings of the Association for Computational Linguistics: ACL 2025</i>, pages 19822–19832, Vienna, Austria. Association for Computational Linguistics.
                    <p><a href="https://aclanthology.org/2025.findings-acl.1017/">[pdf]</a><p>
                </li>
                
                <li>Huiyin Xue and Nikolaos Aletras. 2023. Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention. In <i>Findings of the Association for Computational Linguistics: EMNLP 2023</i>, pages 10355–10373, Sentosa Gateway, Singapore. Association for Computational Linguistics.
                    <p><a href="emnlp23_Poster_.pdf" target="blank">[poster]</a> <a href="https://aclanthology.org/2023.findings-emnlp.695/">[pdf]</a><p>
                </li>
                    
    	    	<li>Huiyin Xue and Nikolaos Aletras. 2022. HashFormers: Towards Vocabulary-independent Pre-trained Transformers. In <i>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</i>, pages 7862–7874, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
                    <p><a href="emnlp22_Poster_.pdf" target="blank">[poster]</a> <a href="https://aclanthology.org/2022.emnlp-main.536/">[pdf]</a> <a href="https://github.com/HUIYINXUE/hashformer">[code]</a><p>
                </li>

                <ul>
    	    </div>

            <h3>THESIS</h3>
            <div class="publication-list">
                <ul>
                <li>Huiyin Xue. 2026. PhD Thesis, University of Sheffield.
                    <p><a href="https://etheses.whiterose.ac.uk/id/eprint/38089/">[pdf]</a><p>
                </li>
                <ul>
    	    </div>
        </div>
    	
    </body>

    <style>
    	body, html{
    		height: 100%;
    	}

    	.publication {
    		margin: 0;
    		position: relative;
    		width: 100%;
    		height: 100%;
    		flex: auto;
    	}

        .small-octopus-img {
        	left: 0%;
        	top: 5%;
        	width: 10%;
        }

        .publication-list {
            position: relative;
            top: 0%;
            left: 10%;
            width: 80%;
            font-size: 20px;
            display: flex;
            flex-wrap: nowrap;
        }

        .publication-list > ul {
            font-size: 20px;
            width: 100%;
            color: black;
        }


    	h2 {
            position: relative;
            top: -15%;
            left: 20%;
    		text-align: left;
    		font-style: normal;
    		font-size: 35px;
    		color: grey;
    	}

        h3 {
            position: relative;
            top: 0%;
            left: 10%;
    		text-align: left;
    		font-style: normal;
    		font-size: 25px;
    		color: grey;
    	}
    </style>

</html>
